{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05ab26f4",
   "metadata": {},
   "source": [
    "1. What are the key reasons for reducing the dimensionality of a dataset? What are the major disadvantages?\n",
    ":It reduces the time and storage space required. It helps Remove multi-collinearity which improves the interpretation of the parameters of the machine learning model. It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D.\n",
    "Dimensionality reduction refers to techniques that reduce the number of input variables in a dataset. More input features often make a predictive modeling task more challenging to model, more generally referred to as the curse of dimensionality.\n",
    "\n",
    "2. What is the dimensionality curse?\n",
    ": It indicates that the number of samples needed to estimate an arbitrary function with a given level of accuracy grows exponentially with respect to the number of input variables (i.e., dimensionality) of the function.\n",
    "\n",
    "3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how can you go about doing it? If not, what is the reason?\n",
    ":dimensionality reduction is not reversible in general.\n",
    "\n",
    "4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?\n",
    ":Depends on dataset. If it is comprised of points that are perfectly aligned, PCA can reduce the dataset down to 1 dimension and preserve 95% of the variance.\n",
    "\n",
    "5. Assume you're running PCA on a 1,000-dimensional dataset with a 95 percent explained variance ratio. What is the number of dimensions that the resulting dataset would have?\n",
    ":\n",
    "\n",
    "6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?\n",
    ":RANDOMIZED PCA is a classical PCA uses the low-rank matrix approximation to estimate the principal components,good for small datasets.Incremental PCA can be used when the dataset is too large to fit in the memory. Kernal PCA  works great for linearly separable datasets. \n",
    "\n",
    "7. How do you assess a dimensionality reduction algorithm's success on your dataset?\n",
    ": \n",
    "\n",
    "8. Is it logical to use two different dimensionality reduction algorithms in a chain?\n",
    ": It can make sense to combine two DR methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
