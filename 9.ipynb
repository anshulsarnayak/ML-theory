{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dda2add",
   "metadata": {},
   "source": [
    "1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth.\n",
    ":  Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.It helps with data clean\n",
    "Feature engineering consists of creation, transformation, extraction, and selection of features, also known as variables, that are most conducive to creating an accurate ML algorithm.\n",
    "\n",
    "2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?\n",
    ":Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features.\n",
    "The objective of feature selection is to remove irrelevant and/or redundant features and retain only relevant features. Irrelevant features can be removed without affecting learning performance. Redundant features are a type of irrelevant features.\n",
    "\n",
    "\n",
    "3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?\n",
    ":\n",
    "Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "\n",
    "4.\n",
    "i. Describe the overall feature selection process.\n",
    "The feature selection process is based on a specific machine learning algorithm that we are trying to fit on a given dataset. It follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion.\n",
    "\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\n",
    ":feature extraction is a general term for methods of constructing combinations of the variables to get around these problems while still describing the data with sufficient accuracy. Many machine learning practitioners believe that properly optimized feature extraction is the key to effective model construction.\n",
    "\n",
    "5. Describe the feature engineering process in the sense of a text categorization issue.\n",
    ":The most important part of text classification is feature engineering: the process of creating features for a machine learning model from raw text data.\n",
    "\n",
    "6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of \n",
    "(2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in  cosine.\n",
    ":Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. It is often used to measure document similarity in text analysis.\n",
    "document-term matrix  two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1) will have cosine resembalance of 0.617\n",
    "\n",
    "7.\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    ":the Hamming distance between two vectors is the number of bits we must change to change one into the other. here Between 10001011 and 11001111 hamming distance is 2.\n",
    "\n",
    "ii. Compare the Jaccard index and sim\n",
    "ilarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n",
    ":The Jaccard similarity index (sometimes called the Jaccard similarity coefficient) compares members for two sets to see which members are shared and which are distinct. It's a measure of similarity for the two sets of data, with a range from 0% to 100%. The higher the percentage, the more similar the two populations.\n",
    "\n",
    "8. State what is meant by  \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?\n",
    ": High dimensional data refers to a dataset in which the number of features p is larger than the number of observations N , For example, tomographic imaging data, ECG data, and MEG data.\n",
    "when the dimensionality increases, the volume of the space increases so fast that the available data become sparse.\n",
    "There are two common ways to deal with high dimensional data:\n",
    "1.Choose to include fewer features. The most obvious way to avoid dealing with high dimensional data is to simply include fewer features in the dataset.\n",
    "2.Use a regularization method\n",
    "\n",
    "9. Make a few quick notes on:\n",
    "\n",
    "1. PCA is an acronym for Personal Computer Analysis.\n",
    ":Principal Component Analysis (PCA) is a statistical procedure that uses an orthogonal transformation that converts a set of correlated variables to a set of uncorrelated variables. PCA is the most widely used tool in exploratory data analysis and in machine learning for predictive models.\n",
    "\n",
    "2. Use of vectors\n",
    ":Vectors are commonly used in machine learning as they lend a convenient way to organize data. Often one of the very first steps in making a machine learning model is vectorizing the data.\n",
    "\n",
    "3. Embedded technique\n",
    ": In the context of machine learning, an embedding is a low-dimensional, learned continuous vector representation of discrete variables into which you can translate high-dimensional vectors. Generally, embeddings make ML models more efficient and easier to work with, and can be used with other models as well.\n",
    "\n",
    "10. Make a comparison between:\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    ":Sequential floating forward selection (SFFS) starts from the empty set. After each forward step, SFFS performs backward steps as long as the objective function increases. Sequential floating backward selection (SFBS) starts from the full set.\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper\n",
    ":Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "\n",
    "3. SMC vs. Jaccard coefficient\n",
    ": the SMC counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent in both sets) as matches and compares it to the total number of attributes in the universe, whereas the Jaccard index only counts mutual presence as matches and compares it to the number of attributes that have been chosen by at least one of the two sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
